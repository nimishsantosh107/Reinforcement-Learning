{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KLAOgs3gOPy8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import gym\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import torch as T\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1619,
     "status": "ok",
     "timestamp": 1596816577313,
     "user": {
      "displayName": "Nimish S",
      "photoUrl": "",
      "userId": "16860995465865824316"
     },
     "user_tz": -330
    },
    "id": "rZDPJ0g-e6fh",
    "outputId": "25bd6f03-aade-4d43-f9aa-524ecee5265b"
   },
   "outputs": [],
   "source": [
    "# T.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9u_bVI_99Rfv"
   },
   "source": [
    "## **Wrappers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L5aHWPN-OEAJ"
   },
   "outputs": [],
   "source": [
    "# PREPROCESS EACH FRAME\n",
    "class PreprocessFrames(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    PREPROCESSES EACH FRAME (input = (rows, columns, 3)) [0-255]\n",
    "    1. convert to grayscale (3 channels to 1)   -   (rows, columns, 1)  [0-255]\n",
    "    2. resize to new shape                      -   (new_rows, new_columns)  [0-255]\n",
    "    3. convert to nparray & reshape             -   array(1, new_rows, new_columns)  [0-255]\n",
    "    4. scale values from 0-1                    -   array(1, new_rows, new_columns)  [0.0-1.0]\n",
    "    \"\"\"\n",
    "    def __init__(self, env, new_observation_shape):\n",
    "        super().__init__(env)\n",
    "        self.new_observation_shape = new_observation_shape\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=self.new_observation_shape, dtype=np.float32)\n",
    "    \n",
    "    def observation(self, observation):\n",
    "        temp_frame = cv2.cvtColor(observation, cv2.COLOR_RGB2GRAY)\n",
    "        temp_frame = cv2.resize(temp_frame, self.new_observation_shape[1:], interpolation=cv2.INTER_AREA)\n",
    "        new_observation = np.array(temp_frame).reshape(self.new_observation_shape)\n",
    "        new_observation = new_observation / 255.0 \n",
    "        return new_observation\n",
    "\n",
    "# TO BE CALLED ON EACH SINGLE IMAGE (AFTER PREPROCESS)\n",
    "class CustomStep(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    OVERRIDES step() & reset()\n",
    "    1. repeats same action in 'n' skipped frames to compute faster.\n",
    "    2. removes flicker in frames by taking max of 2 consecutive frames.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, frame_skip, clip_reward, no_ops, fire_first):\n",
    "        super().__init__(env)\n",
    "        self.frame_skip = frame_skip\n",
    "        self.observation_shape = env.observation_space.shape\n",
    "        self.observation_buffer = np.zeros_like((2, self.observation_shape))\n",
    "        # DURING TESTING ONLY\n",
    "        self.clip_reward = clip_reward\n",
    "        self.no_ops = no_ops\n",
    "        self.fire_first = fire_first\n",
    "\n",
    "    def reset(self):\n",
    "        observation = self.env.reset()\n",
    "        # FOR no_ops\n",
    "        no_ops = (np.random.randint(self.no_ops) + 1) if (self.no_ops > 0) else 0\n",
    "        for _ in range(no_ops):\n",
    "            _, _, done, _ = env.step(0) # 0 - NOOP\n",
    "            if done: self.env.reset()\n",
    "        # FOR fire_first\n",
    "        if (self.fire_first):\n",
    "            assert (self.env.get_action_meanings()[0] == 'FIRE')\n",
    "            observation, _, _, _ = env.step(1) # 1 - FIRE\n",
    "        self.observation_buffer = np.zeros_like((2, self.observation_shape))\n",
    "        self.observation_buffer[0] = observation\n",
    "        return observation\n",
    "\n",
    "    # RETURN FRAME_SKIPPED & FLICKER REMOVED FRAMES \n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "\n",
    "        for frame in range(self.frame_skip):\n",
    "            observation, reward, done, info = self.env.step(action)\n",
    "            # CLIP REWARD (-1,1) IF true\n",
    "            reward = reward if (not self.clip_reward) else np.clip(reward, -1,1)\n",
    "            total_reward += reward\n",
    "\n",
    "            idx = frame % 2\n",
    "            self.observation_buffer[idx] = observation\n",
    "\n",
    "            if done: break\n",
    "\n",
    "        observation_max = np.maximum(self.observation_buffer[0], self.observation_buffer[1])\n",
    "        return observation_max, total_reward, done, info\n",
    "\n",
    "\n",
    "# STACK OBSERVATIONS\n",
    "class StackFrames(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    STACKS stack_size FRAMES TOGETHER AND RETURNS AS THE 'observation'\n",
    "    1. on reset() returns first 'observation' STACKED 'stack_size' times\n",
    "    2. observation() returns current 'observation' STACKED with 'stack_size-1' previous 'observation'\n",
    "    \"\"\"\n",
    "    def __init__(self, env, stack_size):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "                                    env.observation_space.low.repeat(stack_size, axis=0),\n",
    "                                    env.observation_space.high.repeat(stack_size, axis=0)\n",
    "                                 )\n",
    "        self.stack = collections.deque(maxlen=stack_size)\n",
    "\n",
    "    def reset(self):\n",
    "        self.stack.clear()\n",
    "        observation = self.env.reset()\n",
    "        for _ in range(self.stack.maxlen):\n",
    "            self.stack.append(observation)\n",
    "        observation = np.array(self.stack).reshape(self.observation_space.shape)\n",
    "        return observation\n",
    "        \n",
    "    def observation(self, observation):\n",
    "        self.stack.append(observation)\n",
    "        observation = np.array(self.stack).reshape(self.observation_space.shape)\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R9YVrXJ0A0nd"
   },
   "outputs": [],
   "source": [
    "# TIE EVERYTHING TOGETHER\n",
    "def make_env(env_name, new_observation_shape=(1,84,84), stack_size=4, frame_skip=4, clip_reward=False, no_ops=0, fire_first=False):\n",
    "    env = gym.make(env_name)\n",
    "    env = PreprocessFrames(env, new_observation_shape=new_observation_shape)\n",
    "    env = CustomStep(env, frame_skip=4, clip_reward=clip_reward, no_ops=no_ops, fire_first=fire_first)\n",
    "    env = StackFrames(env, stack_size=stack_size)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j4SkkGZU9Dd_"
   },
   "source": [
    "## **ReplayBuffer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-SvHE9YKo0r-"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, mem_size, observation_shape, n_actions):\n",
    "        self.mem_size = mem_size\n",
    "        self.mem_counter = 0\n",
    "        # DATA\n",
    "        self.states = np.zeros((mem_size, *observation_shape), dtype=np.float32)\n",
    "        self.actions = np.zeros(mem_size, dtype=np.int64)\n",
    "        self.rewards = np.zeros(mem_size, dtype=np.int64)\n",
    "        self.states_ = np.zeros((mem_size, *observation_shape), dtype=np.float32)\n",
    "        self.terminals = np.zeros(mem_size, dtype=bool)\n",
    "\n",
    "    # STORE TRANSITIONS IN BUFFER\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        index = self.mem_counter % self.mem_size\n",
    "        self.states[index] = state\n",
    "        self.actions[index] = action\n",
    "        self.rewards[index] = reward\n",
    "        self.states_[index] = state_\n",
    "        self.terminals[index] = done # 1 if 'done' else 0\n",
    "        self.mem_counter += 1\n",
    "\n",
    "    # UNIFORMLY SAMPLES 'BUFFER' AND RETURNS A 'BATCH' OF batch_size\n",
    "    def sample_batch(self, batch_size):\n",
    "        max_index = min(self.mem_counter, self.mem_size)\n",
    "        batch_indices = np.random.choice(max_index, batch_size, replace=False)\n",
    "        states = self.states[batch_indices]\n",
    "        actions = self.actions[batch_indices]\n",
    "        rewards = self.rewards[batch_indices]\n",
    "        states_ = self.states_[batch_indices]\n",
    "        terminals = self.terminals[batch_indices]\n",
    "        return (states, actions, rewards, states_, terminals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B9mN3hsv9Hl2"
   },
   "source": [
    "## **Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N4-yaJCQ0Kci"
   },
   "outputs": [],
   "source": [
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, lr, observation_shape, n_actions, model_name, model_dir):\n",
    "        super().__init__()\n",
    "        self.model_dir = model_dir\n",
    "        self.model_file = os.path.join(self.model_dir, model_name)\n",
    "        # CNN\n",
    "        self.conv1 = nn.Conv2d(observation_shape[0], 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        # CNN -> ANN\n",
    "        fc_input_dims = self.caculate_conv_output_dims(observation_shape)\n",
    "        # ANN\n",
    "        self.fc1 = nn.Linear(fc_input_dims, 512)\n",
    "        self.out = nn.Linear(512, n_actions)\n",
    "        # UTILS\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.optimizer = optim.RMSprop(self.parameters(), lr=lr)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.to(self.device)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        t = F.relu(self.conv1(state))\n",
    "        t = F.relu(self.conv2(t))\n",
    "        t = F.relu(self.conv3(t))\n",
    "        t = F.relu(self.fc1(t.reshape(t.shape[0], -1)))\n",
    "        q_values = self.out(t)\n",
    "        return q_values\n",
    "\n",
    "    def caculate_conv_output_dims(self, observation_shape):\n",
    "        dims = T.zeros((1, *observation_shape))\n",
    "        dims = self.conv1(dims)\n",
    "        dims = self.conv2(dims)\n",
    "        dims = self.conv3(dims)\n",
    "        return int(np.prod(dims.shape))\n",
    "\n",
    "    def save_model(self):\n",
    "        print(\"[INFO] Saving model\")\n",
    "        checkpoint = {\n",
    "            'model_state_dict': self.state_dict(),\n",
    "            'optimizer_state_dict' : self.optimizer.state_dict()\n",
    "        }\n",
    "        T.save(checkpoint, self.model_file)\n",
    "    \n",
    "    def load_model(self, cpu=False):\n",
    "        print(\"[INFO] Loading model\")\n",
    "        \n",
    "        map_location = T.device('cpu') if (cpu) else None\n",
    "        \n",
    "        checkpoint = T.load(self.model_file, map_location=map_location)\n",
    "        self.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "APhMTaPP9Kxl"
   },
   "source": [
    "## **Agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jf105mZ2pb8I"
   },
   "outputs": [],
   "source": [
    "class DDQNAgent:\n",
    "    def __init__(self, observation_shape, n_actions, lr, gamma, epsilon, epsilon_min, epsilon_decay,\n",
    "                 mem_size, batch_size, Q_TARGET_replace_interval, algo_name, env_name, model_dir):\n",
    "        self.observation_shape = observation_shape\n",
    "        self.n_actions = n_actions\n",
    "        self.LR = lr\n",
    "        self.GAMMA = gamma\n",
    "        self.EPSILON = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "        # MEM PARAMS\n",
    "        self.mem_size = mem_size\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = ReplayBuffer(mem_size, observation_shape, n_actions)\n",
    "\n",
    "        # MODEL PARAMS\n",
    "        self.learn_counter = 0 # TO UPDATE TARGET NETWORK\n",
    "        self.algo_name = algo_name\n",
    "        self.env_name = env_name\n",
    "        self.model_dir = model_dir\n",
    "        self.Q_TARGET_replace_interval = Q_TARGET_replace_interval\n",
    "        # Q1\n",
    "        self.Q_STEP = DeepQNetwork(lr, observation_shape, n_actions,\n",
    "                              model_name = env_name+'_'+algo_name+'_Q_STEP',\n",
    "                              model_dir = model_dir)\n",
    "        # Q2\n",
    "        self.Q_TARGET = DeepQNetwork(lr, observation_shape, n_actions,\n",
    "                              model_name = env_name+'_'+algo_name+'_Q_TARGET',\n",
    "                              model_dir = model_dir)\n",
    "\n",
    "    # e-GREEDY POLICY\n",
    "    def get_action(self, observation, greedy=False):\n",
    "        if ( (np.random.uniform() >= self.EPSILON) or greedy):\n",
    "            observation = T.tensor(observation, dtype=T.float32).to(self.Q_STEP.device)\n",
    "            state = T.unsqueeze(observation, 0)\n",
    "            actions = self.Q_STEP(state)\n",
    "            action = T.argmax(actions).item()\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        return action\n",
    "\n",
    "    def learn(self):\n",
    "        if (self.memory.mem_counter < self.batch_size): return # return if insufficient samples present\n",
    "        # RESET TARGET NETWORK (1 / 1000)\n",
    "        self.update_Q_TARGET()\n",
    "\n",
    "        # PREDICT Q1(s,a)\n",
    "        states, actions, rewards, states_, terminals = self.sample_batch()\n",
    "        q1 = self.Q_STEP(states)         # q - batch_size * n_actions\n",
    "        indices = np.arange(len(actions))\n",
    "        q1_preds = q1[indices, actions]\n",
    "\n",
    "        # GET Q1(s_,A) and Q2(s_,A)\n",
    "        q1_ = self.Q_STEP(states_)\n",
    "        q2_ = self.Q_TARGET(states_)\n",
    "        # argmax(Q1(s_,A)) - (max)a_\n",
    "        # Q2(s_, (max)a_) - TARGETS\n",
    "        a_ = T.argmax(q1_, dim=1)\n",
    "        indices = np.arange(len(a_))\n",
    "        q2_next = q2_[indices, a_]\n",
    "        q2_next[terminals] = 0.0         # Q2(s_) = 0 where terminal=1\n",
    "        q2_targets = rewards + (self.GAMMA * q2_next)\n",
    "\n",
    "        # CALC LOSS & BACKPROP\n",
    "        loss = self.Q_STEP.loss(q2_targets, q1_preds).to(self.Q_STEP.device)\n",
    "        self.Q_STEP.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.Q_STEP.optimizer.step()\n",
    "\n",
    "        self.learn_counter += 1\n",
    "        self.decay_epsilon()\n",
    "\n",
    "    def update_Q_TARGET(self):\n",
    "        if ((self.learn_counter % self.Q_TARGET_replace_interval) == 0):\n",
    "            self.Q_TARGET.load_state_dict(self.Q_STEP.state_dict())\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        if (self.EPSILON > self.epsilon_min):\n",
    "            self.EPSILON -= self.epsilon_decay\n",
    "        else:\n",
    "            self.EPSILON = self.epsilon_min\n",
    "    \n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        self.memory.store_transition(state, action, reward, state_, done)\n",
    "\n",
    "    def sample_batch(self):\n",
    "        states, actions, rewards, states_, terminals = self.memory.sample_batch(self.batch_size)\n",
    "        states = T.tensor(states).to(self.Q_STEP.device)\n",
    "        actions = T.tensor(actions).to(self.Q_STEP.device)\n",
    "        rewards = T.tensor(rewards).to(self.Q_STEP.device)\n",
    "        states_ = T.tensor(states_).to(self.Q_STEP.device)\n",
    "        terminals = T.tensor(terminals).to(self.Q_STEP.device)\n",
    "        return states, actions, rewards, states_, terminals\n",
    "        \n",
    "    def save_models(self):\n",
    "        self.Q_STEP.save_model()\n",
    "        self.Q_TARGET.save_model()\n",
    "    \n",
    "    def load_models(self, cpu=False):\n",
    "        self.Q_STEP.load_model(cpu)\n",
    "        self.Q_TARGET.load_model(cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g7HQrV3WcH2c"
   },
   "source": [
    "## **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRAINING ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mj6YZi0aXsDX"
   },
   "outputs": [],
   "source": [
    "env_name = 'PongNoFrameskip-v4'\n",
    "env = make_env(env_name)\n",
    "\n",
    "N_EPISODES = 500\n",
    "\n",
    "agent = DDQNAgent(observation_shape=env.observation_space.shape,\n",
    "                  n_actions=env.action_space.n,\n",
    "                  lr=1e-4,\n",
    "                  gamma=0.99,\n",
    "                  epsilon=1.0,\n",
    "                  epsilon_min=0.06,\n",
    "                  epsilon_decay=1e-5,\n",
    "                  mem_size=25000,\n",
    "                  batch_size=128,\n",
    "                  Q_TARGET_replace_interval=1000,\n",
    "                  algo_name='DDQN',\n",
    "                  env_name=env_name,\n",
    "                  model_dir='./weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "553f3f2b4e164ccfbc5ddfd2d79bcdfa",
      "c74e563a80954025bf54f09f6ddab1ef",
      "800189801a7b4df081c2d1212396f440",
      "aa8baad31c564581a9bb59e9246401be",
      "8097a02df891400c914eb8d624256c91",
      "0b41cf3738f94d26b2b02c8caf929843",
      "db086016d7fb441b91078372660ae2df",
      "709e6b240ab145f5a14d30a2955cfb2c"
     ]
    },
    "colab_type": "code",
    "id": "Rp2We9miezCT",
    "outputId": "103b09cc-6906-410f-cd36-2fb46bcc1869"
   },
   "outputs": [],
   "source": [
    "episode_rewards, episode_lengths, episode_epsilons, mean_rewards = [],[],[],[]\n",
    "best_reward = -np.inf\n",
    "\n",
    "for episode_n in tqdm(range(N_EPISODES)):\n",
    "    total_reward, total_moves = 0,0\n",
    "\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "\n",
    "    while not done:\n",
    "        # e_GREEDY ACTION\n",
    "        action = agent.get_action(observation)\n",
    "        observation_, reward, done, _ = env.step(action)\n",
    "\n",
    "        total_reward += reward\n",
    "        total_moves += 1\n",
    "\n",
    "        # STORE DATA & LEARN\n",
    "        agent.store_transition(observation, action, reward, observation_, done)\n",
    "        agent.learn()\n",
    "\n",
    "        observation = observation_\n",
    "\n",
    "    episode_rewards.append(total_reward)\n",
    "    episode_lengths.append(total_moves)\n",
    "    episode_epsilons.append(agent.EPSILON)\n",
    "\n",
    "    mean_reward = np.mean(episode_rewards[-100:])\n",
    "    mean_rewards.append(mean_reward)\n",
    "    if(mean_reward > best_reward):\n",
    "        agent.save_models()\n",
    "        best_reward = mean_reward\n",
    "\n",
    "    print(\"ITER: \",episode_n,\"\\tRWD: \",total_reward,\"\\tMEAN_RWD: \",round(mean_reward,2),\"\\tLEN: \",total_moves,\"\\tEPS: \",round(agent.EPSILON,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'PongNoFrameskip-v4'\n",
    "env = make_env(env_name)\n",
    "\n",
    "agent = DQNAgent(observation_shape=env.observation_space.shape,\n",
    "                 n_actions=env.action_space.n,\n",
    "                 lr=1e-4,\n",
    "                 gamma=0.99,\n",
    "                 epsilon=0.001,\n",
    "                 epsilon_min=0.001,\n",
    "                 epsilon_decay=1e-5,\n",
    "                 mem_size=1,\n",
    "                 batch_size=1,\n",
    "                 Q_TARGET_replace_interval=1000,\n",
    "                 algo_name='DDQN',\n",
    "                 env_name=env_name,\n",
    "                 model_dir='./weights')\n",
    "                 \n",
    "agent.load_models(cpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with T.no_grad():\n",
    "    total_reward, total_moves = 0,0\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "\n",
    "    while not done:\n",
    "        time.sleep(0.0001)\n",
    "        env.render()\n",
    "\n",
    "        # e_GREEDY ACTION\n",
    "        action = agent.get_action(observation, greedy=True)\n",
    "        observation_, reward, done, _ = env.step(action)\n",
    "\n",
    "        total_reward += reward\n",
    "        total_moves += 1\n",
    "\n",
    "        observation = observation_\n",
    "    print(\"RWD: \",total_reward,\"\\tLEN: \",total_moves)\n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNoAQ4Rpbc2gTJZJ+hdiCPH",
   "collapsed_sections": [
    "9u_bVI_99Rfv",
    "j4SkkGZU9Dd_",
    "B9mN3hsv9Hl2",
    "APhMTaPP9Kxl"
   ],
   "name": "DQN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('reinforcement-learning': conda)",
   "name": "python_defaultSpec_1596826856201"
  },
  "language_info": {
   "name": ""
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0b41cf3738f94d26b2b02c8caf929843": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "553f3f2b4e164ccfbc5ddfd2d79bcdfa": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_800189801a7b4df081c2d1212396f440",
       "IPY_MODEL_aa8baad31c564581a9bb59e9246401be"
      ],
      "layout": "IPY_MODEL_c74e563a80954025bf54f09f6ddab1ef"
     }
    },
    "709e6b240ab145f5a14d30a2955cfb2c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "800189801a7b4df081c2d1212396f440": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": " 48%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0b41cf3738f94d26b2b02c8caf929843",
      "max": 500,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8097a02df891400c914eb8d624256c91",
      "value": 242
     }
    },
    "8097a02df891400c914eb8d624256c91": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "aa8baad31c564581a9bb59e9246401be": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_709e6b240ab145f5a14d30a2955cfb2c",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_db086016d7fb441b91078372660ae2df",
      "value": " 242/500 [2:41:18&lt;2:53:42, 40.40s/it]"
     }
    },
    "c74e563a80954025bf54f09f6ddab1ef": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "db086016d7fb441b91078372660ae2df": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
