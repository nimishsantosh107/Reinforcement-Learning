{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DDQN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "9u_bVI_99Rfv"
      ]
    },
    "kernelspec": {
      "name": "python_defaultSpec_1597012702101",
      "display_name": "Python 3.6.10 64-bit ('reinforcement-learning': conda)"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1012498d1d6345808044f554a5957717": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_612cab01ce0c463e90be4526db949a4a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_577552484f1b46ee807fbf522df8e0c7",
              "IPY_MODEL_c3e3e3d4a2c242ea85e6b8f37ff96ff1"
            ]
          }
        },
        "612cab01ce0c463e90be4526db949a4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "577552484f1b46ee807fbf522df8e0c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2cd882d56450458b99dc64850dccb6d2",
            "_dom_classes": [],
            "description": "  8%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 2000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 166,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bac2d5c8db6f4a3283d5e1878b543e11"
          }
        },
        "c3e3e3d4a2c242ea85e6b8f37ff96ff1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fb6206f1dc254007bbd7c35b7ab2de43",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 166/2000 [06:29&lt;3:07:09,  6.12s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d159c6562f0240638670aa7b33c5553b"
          }
        },
        "2cd882d56450458b99dc64850dccb6d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bac2d5c8db6f4a3283d5e1878b543e11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fb6206f1dc254007bbd7c35b7ab2de43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d159c6562f0240638670aa7b33c5553b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLAOgs3gOPy8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import gym\n",
        "import time\n",
        "import collections\n",
        "\n",
        "import numpy as np\n",
        "import torch as T\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZDPJ0g-e6fh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "a4be8f88-ce37-4151-cf56-f4c0f9cc73e3"
      },
      "source": [
        "# T.cuda.get_device_name()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4SkkGZU9Dd_",
        "colab_type": "text"
      },
      "source": [
        "## **ReplayBuffer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SvHE9YKo0r-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, mem_size, observation_shape, n_actions):\n",
        "        self.mem_size = mem_size\n",
        "        self.mem_counter = 0\n",
        "        # DATA\n",
        "        self.states = np.zeros((mem_size, *observation_shape), dtype=np.float32)\n",
        "        self.actions = np.zeros(mem_size, dtype=np.int64)\n",
        "        self.rewards = np.zeros(mem_size, dtype=np.int64)\n",
        "        self.states_ = np.zeros((mem_size, *observation_shape), dtype=np.float32)\n",
        "        self.terminals = np.zeros(mem_size, dtype=bool)\n",
        "\n",
        "    # STORE TRANSITIONS IN BUFFER\n",
        "    def store_transition(self, state, action, reward, state_, done):\n",
        "        index = self.mem_counter % self.mem_size\n",
        "        self.states[index] = state\n",
        "        self.actions[index] = action\n",
        "        self.rewards[index] = reward\n",
        "        self.states_[index] = state_\n",
        "        self.terminals[index] = done # 1 if 'done' else 0\n",
        "        self.mem_counter += 1\n",
        "\n",
        "    # UNIFORMLY SAMPLES 'BUFFER' AND RETURNS A 'BATCH' OF batch_size\n",
        "    def sample_batch(self, batch_size):\n",
        "        max_index = min(self.mem_counter, self.mem_size)\n",
        "        batch_indices = np.random.choice(max_index, batch_size, replace=False)\n",
        "        states = self.states[batch_indices]\n",
        "        actions = self.actions[batch_indices]\n",
        "        rewards = self.rewards[batch_indices]\n",
        "        states_ = self.states_[batch_indices]\n",
        "        terminals = self.terminals[batch_indices]\n",
        "        return (states, actions, rewards, states_, terminals)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9mN3hsv9Hl2",
        "colab_type": "text"
      },
      "source": [
        "## **Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4-yaJCQ0Kci",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DuelingDeepQNetwork(nn.Module):\n",
        "    def __init__(self, lr, observation_shape, n_actions, model_name, model_dir):\n",
        "        super().__init__()\n",
        "        self.model_dir = model_dir\n",
        "        self.model_file = os.path.join(self.model_dir, model_name)\n",
        "        # ANN\n",
        "        self.fc1 = nn.Linear(observation_shape[0], 512)\n",
        "        self.fc2 = nn.Linear(512, 1024)\n",
        "        self.fc3 = nn.Linear(1024, 256)\n",
        "        # DUELING\n",
        "        self.V = nn.Linear(256, 1)\n",
        "        self.A = nn.Linear(256, n_actions)\n",
        "        # UTILS\n",
        "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
        "        self.optimizer = optim.RMSprop(self.parameters(), lr=lr)\n",
        "        self.loss = nn.MSELoss()\n",
        "        self.to(self.device)\n",
        "    \n",
        "    def forward(self, state):\n",
        "        t = F.relu(self.fc1(state))\n",
        "        t = F.relu(self.fc2(t))\n",
        "        t = F.relu(self.fc3(t))\n",
        "        V = self.V(t)\n",
        "        A = self.A(t)\n",
        "        return V,A\n",
        "\n",
        "    def save_model(self):\n",
        "        print(\"[INFO] Saving model\")\n",
        "        checkpoint = {\n",
        "            'model_state_dict': self.state_dict(),\n",
        "            'optimizer_state_dict' : self.optimizer.state_dict()\n",
        "        }\n",
        "        T.save(checkpoint, self.model_file)\n",
        "    \n",
        "    def load_model(self, cpu=False):\n",
        "        print(\"[INFO] Loading model\")\n",
        "        \n",
        "        map_location = T.device('cpu') if (cpu) else None\n",
        "        \n",
        "        checkpoint = T.load(self.model_file, map_location=map_location)\n",
        "        self.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APhMTaPP9Kxl",
        "colab_type": "text"
      },
      "source": [
        "## **Agent**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jf105mZ2pb8I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DuelingDDQNAgent:\n",
        "    def __init__(self, observation_shape, n_actions, lr, gamma, epsilon, epsilon_min, epsilon_decay,\n",
        "                 mem_size, batch_size, Q_TARGET_replace_interval, algo_name, env_name, model_dir):\n",
        "        self.observation_shape = observation_shape\n",
        "        self.n_actions = n_actions\n",
        "        self.LR = lr\n",
        "        self.GAMMA = gamma\n",
        "        self.EPSILON = epsilon\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "\n",
        "        # MEM PARAMS\n",
        "        self.mem_size = mem_size\n",
        "        self.batch_size = batch_size\n",
        "        self.memory = ReplayBuffer(mem_size, observation_shape, n_actions)\n",
        "\n",
        "        # MODEL PARAMS\n",
        "        self.learn_counter = 0 # TO UPDATE TARGET NETWORK\n",
        "        self.algo_name = algo_name\n",
        "        self.env_name = env_name\n",
        "        self.model_dir = model_dir\n",
        "        self.Q_TARGET_replace_interval = Q_TARGET_replace_interval\n",
        "        # Q1\n",
        "        self.Q_STEP = DuelingDeepQNetwork(lr, observation_shape, n_actions,\n",
        "                              model_name = env_name+'_'+algo_name+'_Q_STEP',\n",
        "                              model_dir = model_dir)\n",
        "        # Q2\n",
        "        self.Q_TARGET = DuelingDeepQNetwork(lr, observation_shape, n_actions,\n",
        "                              model_name = env_name+'_'+algo_name+'_Q_TARGET',\n",
        "                              model_dir = model_dir)\n",
        "\n",
        "    # e-GREEDY POLICY\n",
        "    def get_action(self, observation, greedy=False):\n",
        "        if ( (np.random.uniform() >= self.EPSILON) or greedy):\n",
        "            observation = T.tensor(observation, dtype=T.float32).to(self.Q_STEP.device)\n",
        "            state = T.unsqueeze(observation, 0)\n",
        "            _,A = self.Q_STEP(state)\n",
        "            action = T.argmax(A).item()\n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "        return action\n",
        "\n",
        "    def learn(self):\n",
        "        if (self.memory.mem_counter < self.batch_size): return # return if insufficient samples present\n",
        "        # RESET TARGET NETWORK (1 / 1000)\n",
        "        self.update_Q_TARGET()\n",
        "\n",
        "        states, actions, rewards, states_, terminals = self.sample_batch()\n",
        "        # PREDICT Q1(s,a)\n",
        "        v1,a1 = self.Q_STEP(states)\n",
        "        q1 = v1 + (a1 - a1.mean(dim=1, keepdim=True)) # q - batch_size * n_actions\n",
        "        indices = np.arange(len(actions))\n",
        "        q1_preds = q1[indices,actions]\n",
        "\n",
        "        # GET V1,A2(s_,A) and V2,A2(s_,A)\n",
        "        v1_, a1_ = self.Q_STEP(states_)\n",
        "        v2_, a2_ = self.Q_TARGET(states_)\n",
        "        # GET Q1(s_,A) and Q2(s_,A)\n",
        "        q1_ = v1_ + (a1_ - a1_.mean(dim=1, keepdim=True))\n",
        "        q2_ = v2_ + (a2_ - a2_.mean(dim=1, keepdim=True))\n",
        "        # argmax(Q1(s_,A)) - (max)a_\n",
        "        # Q2(s_, (max)a_) - TARGETS\n",
        "        a_ = T.argmax(q1_, dim=1)\n",
        "        indices = np.arange(len(a_))\n",
        "        q2_next = q2_[indices, a_]\n",
        "        q2_next[terminals] = 0.0                      # Q2(s_) = 0 where terminal=1\n",
        "        q2_targets = rewards + (self.GAMMA * q2_next)\n",
        "\n",
        "        # CALC LOSS & BACKPROP\n",
        "        loss = self.Q_STEP.loss(q2_targets, q1_preds).to(self.Q_STEP.device)\n",
        "        self.Q_STEP.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.Q_STEP.optimizer.step()\n",
        "\n",
        "        self.learn_counter += 1\n",
        "        self.decay_epsilon()\n",
        "\n",
        "    def update_Q_TARGET(self):\n",
        "        if ((self.learn_counter % self.Q_TARGET_replace_interval) == 0):\n",
        "            self.Q_TARGET.load_state_dict(self.Q_STEP.state_dict())\n",
        "    \n",
        "    def decay_epsilon(self):\n",
        "        if (self.EPSILON > self.epsilon_min):\n",
        "            self.EPSILON -= self.epsilon_decay\n",
        "        else:\n",
        "            self.EPSILON = self.epsilon_min\n",
        "    \n",
        "    def store_transition(self, state, action, reward, state_, done):\n",
        "        self.memory.store_transition(state, action, reward, state_, done)\n",
        "\n",
        "    def sample_batch(self):\n",
        "        states, actions, rewards, states_, terminals = self.memory.sample_batch(self.batch_size)\n",
        "        states = T.tensor(states).to(self.Q_STEP.device)\n",
        "        actions = T.tensor(actions).to(self.Q_STEP.device)\n",
        "        rewards = T.tensor(rewards).to(self.Q_STEP.device)\n",
        "        states_ = T.tensor(states_).to(self.Q_STEP.device)\n",
        "        terminals = T.tensor(terminals).to(self.Q_STEP.device)\n",
        "        return states, actions, rewards, states_, terminals\n",
        "        \n",
        "    def save_models(self):\n",
        "        self.Q_STEP.save_model()\n",
        "        self.Q_TARGET.save_model()\n",
        "    \n",
        "    def load_models(self, cpu=False):\n",
        "        self.Q_STEP.load_model(cpu)\n",
        "        self.Q_TARGET.load_model(cpu)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7HQrV3WcH2c",
        "colab_type": "text"
      },
      "source": [
        "## **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mj6YZi0aXsDX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env_name = \"LunarLander-v2\"\n",
        "env = gym.make(env_name)\n",
        "\n",
        "N_EPISODES = 2000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRpSgQ0OJi9d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "agent = DuelingDDQNAgent(observation_shape=env.observation_space.shape,\n",
        "                 n_actions=env.action_space.n,\n",
        "                 lr=1e-4,\n",
        "                 gamma=0.99,\n",
        "                 epsilon=1.0,\n",
        "                 epsilon_min=0.01,\n",
        "                 epsilon_decay=4e-5,\n",
        "                 mem_size=20000,\n",
        "                 batch_size=512,\n",
        "                 Q_TARGET_replace_interval=1000,\n",
        "                 algo_name='DuelingDDQN',\n",
        "                 env_name=env_name,\n",
        "                 model_dir='./weights')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rp2We9miezCT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1012498d1d6345808044f554a5957717",
            "612cab01ce0c463e90be4526db949a4a",
            "577552484f1b46ee807fbf522df8e0c7",
            "c3e3e3d4a2c242ea85e6b8f37ff96ff1",
            "2cd882d56450458b99dc64850dccb6d2",
            "bac2d5c8db6f4a3283d5e1878b543e11",
            "fb6206f1dc254007bbd7c35b7ab2de43",
            "d159c6562f0240638670aa7b33c5553b"
          ]
        },
        "outputId": "4edf2e2c-2e28-42c4-a587-dbe8b2dd0b9e"
      },
      "source": [
        "episode_rewards, episode_lengths, episode_epsilons, mean_rewards = [],[],[],[]\n",
        "best_reward = -np.inf\n",
        "\n",
        "for episode_n in tqdm(range(N_EPISODES)):\n",
        "    total_reward, total_moves = 0,0\n",
        "\n",
        "    done = False\n",
        "    observation = env.reset()\n",
        "\n",
        "    while not done:\n",
        "        # e_GREEDY ACTION\n",
        "        action = agent.get_action(observation)\n",
        "        observation_, reward, done, _ = env.step(action)\n",
        "\n",
        "        total_reward += reward\n",
        "        total_moves += 1\n",
        "\n",
        "        # STORE DATA & LEARN\n",
        "        agent.store_transition(observation, action, reward, observation_, done)\n",
        "        agent.learn()\n",
        "\n",
        "        observation = observation_\n",
        "\n",
        "    episode_rewards.append(total_reward)\n",
        "    episode_lengths.append(total_moves)\n",
        "    episode_epsilons.append(agent.EPSILON)\n",
        "\n",
        "    mean_reward = np.mean(episode_rewards[-100:])\n",
        "    mean_rewards.append(mean_reward)\n",
        "    if(mean_reward > best_reward):\n",
        "        agent.save_models()\n",
        "        best_reward = mean_reward\n",
        "\n",
        "    print(\"ITER: \",episode_n,\"\\tRWD: \",total_reward,\"\\tM_RWD: \",round(mean_reward,2),\"\\tLEN: \",total_moves,\"\\tEPS: \",round(agent.EPSILON,4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "yMBRuS2vB-3q"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "env_name = 'LunarLander-v2'\n",
        "env = gym.make(env_name)\n",
        "\n",
        "agent = DuelingDDQNAgent(observation_shape=env.observation_space.shape,\n",
        "                 n_actions=env.action_space.n,\n",
        "                 lr=1e-4,\n",
        "                 gamma=0.99,\n",
        "                 epsilon=0.001,\n",
        "                 epsilon_min=0.001,\n",
        "                 epsilon_decay=1e-5,\n",
        "                 mem_size=1,\n",
        "                 batch_size=1,\n",
        "                 Q_TARGET_replace_interval=1000,\n",
        "                 algo_name='DuelingDDQN',\n",
        "                 env_name=env_name,\n",
        "                 model_dir='./weights')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[INFO] Loading model\n[INFO] Loading model\n"
        }
      ],
      "source": [
        "agent.load_models(cpu=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "RWD:  247.21727042772432 \tLEN:  282\n"
        }
      ],
      "source": [
        "with T.no_grad():\n",
        "    total_reward, total_moves = 0,0\n",
        "    done = False\n",
        "    observation = env.reset()\n",
        "\n",
        "    while not done:\n",
        "        time.sleep(0.0001)\n",
        "        env.render()\n",
        "\n",
        "        # e_GREEDY ACTION\n",
        "        action = agent.get_action(observation, greedy=True)\n",
        "        observation_, reward, done, _ = env.step(action)\n",
        "\n",
        "        total_reward += reward\n",
        "        total_moves += 1\n",
        "\n",
        "        observation = observation_\n",
        "    print(\"RWD: \",total_reward,\"\\tLEN: \",total_moves)\n",
        "    env.close()"
      ]
    }
  ]
}