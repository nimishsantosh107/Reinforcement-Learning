{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JeULB07yjFI9"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install box2d-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KLAOgs3gOPy8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import gym\n",
    "import time\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import torch as T\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "rZDPJ0g-e6fh",
    "outputId": "5ae6f5d6-3aaf-48c8-e713-bb7d763e2152"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Tesla T4'"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j4SkkGZU9Dd_"
   },
   "source": [
    "## **ReplayBuffer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-SvHE9YKo0r-"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, mem_size, observation_shape, n_actions, alpha):\n",
    "        self.mem_size = mem_size\n",
    "        self.mem_counter = 0\n",
    "        self.ALPHA = alpha\n",
    "        # DATA\n",
    "        self.states = np.zeros((mem_size, *observation_shape), dtype=np.float32)\n",
    "        self.actions = np.zeros(mem_size, dtype=np.int64)\n",
    "        self.rewards = np.zeros(mem_size, dtype=np.int64)\n",
    "        self.states_ = np.zeros((mem_size, *observation_shape), dtype=np.float32)\n",
    "        self.terminals = np.zeros(mem_size, dtype=bool)\n",
    "        self.priorities = np.zeros(mem_size, dtype=np.float32)\n",
    "\n",
    "    # STORE TRANSITIONS IN BUFFER\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        index = self.mem_counter % self.mem_size\n",
    "        self.states[index] = state\n",
    "        self.actions[index] = action\n",
    "        self.rewards[index] = reward\n",
    "        self.states_[index] = state_\n",
    "        self.terminals[index] = done    # 1 if 'done' else 0\n",
    "        self.priorities[index] = self.priorities.max() if (self.mem_counter>0) else 1.0\n",
    "        self.mem_counter += 1\n",
    "    \n",
    "    # UPDATE PRIORITIES LIST\n",
    "    def update_priotities(self, indices, errors, offset):\n",
    "        priorities = abs(errors) + offset\n",
    "        self.priorities[indices] = priorities\n",
    "\n",
    "    # UNIFORMLY SAMPLES 'BUFFER' AND RETURNS A 'BATCH' OF batch_size\n",
    "    def sample_batch(self, batch_size, beta):\n",
    "        max_index = min(self.mem_counter, self.mem_size) \n",
    "        priorities = self.priorities[:max_index]\n",
    "        probabilities = (priorities ** self.ALPHA) / ((priorities ** self.ALPHA).sum())  # Pr = pi^a/P^a\n",
    "        batch_indices = np.random.choice(max_index, batch_size, p=probabilities)\n",
    "\n",
    "        importance = (max_index * probabilities[batch_indices]) ** (-beta)               # (1/N * 1/Pr)^b\n",
    "        importance = importance / importance.max()\n",
    "        importance = np.array(importance, dtype=np.float32)\n",
    "\n",
    "        states = self.states[batch_indices]\n",
    "        actions = self.actions[batch_indices]\n",
    "        rewards = self.rewards[batch_indices]\n",
    "        states_ = self.states_[batch_indices]\n",
    "        terminals = self.terminals[batch_indices]\n",
    "        return (batch_indices, states, actions, rewards, states_, terminals, importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B9mN3hsv9Hl2"
   },
   "source": [
    "## **Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N4-yaJCQ0Kci"
   },
   "outputs": [],
   "source": [
    "class DuelingDeepQNetwork(nn.Module):\n",
    "    def __init__(self, lr, observation_shape, n_actions, model_name, model_dir):\n",
    "        super().__init__()\n",
    "        self.model_dir = model_dir\n",
    "        self.model_file = os.path.join(self.model_dir, model_name)\n",
    "        # ANN\n",
    "        self.fc1 = nn.Linear(observation_shape[0], 512)\n",
    "        self.fc2 = nn.Linear(512, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 256)\n",
    "        # DUELING\n",
    "        self.V = nn.Linear(256, 1)\n",
    "        self.A = nn.Linear(256, n_actions)\n",
    "        # UTILS\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.optimizer = optim.RMSprop(self.parameters(), lr=lr)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.to(self.device)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        t = F.relu(self.fc1(state))\n",
    "        t = F.relu(self.fc2(t))\n",
    "        t = F.relu(self.fc3(t))\n",
    "        V = self.V(t)\n",
    "        A = self.A(t)\n",
    "        return V,A\n",
    "\n",
    "    def save_model(self):\n",
    "        print(\"[INFO] Saving model\")\n",
    "        checkpoint = {\n",
    "            'model_state_dict': self.state_dict(),\n",
    "            'optimizer_state_dict' : self.optimizer.state_dict()\n",
    "        }\n",
    "        T.save(checkpoint, self.model_file)\n",
    "    \n",
    "    def load_model(self, cpu=False):\n",
    "        print(\"[INFO] Loading model\")\n",
    "        \n",
    "        map_location = T.device('cpu') if (cpu) else None\n",
    "        \n",
    "        checkpoint = T.load(self.model_file, map_location=map_location)\n",
    "        self.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "APhMTaPP9Kxl"
   },
   "source": [
    "## **Agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jf105mZ2pb8I"
   },
   "outputs": [],
   "source": [
    "class DuelingDDQNAgent:\n",
    "    def __init__(self, observation_shape, n_actions, lr, gamma, epsilon, epsilon_min, epsilon_decay, beta, beta_max, beta_increment,\n",
    "                 mem_size, mem_alpha, batch_size, Q_TARGET_replace_interval, warmup, algo_name, env_name, model_dir):\n",
    "        self.observation_shape = observation_shape\n",
    "        self.n_actions = n_actions\n",
    "        self.LR = lr\n",
    "        self.GAMMA = gamma\n",
    "        self.EPSILON = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        # MEM PARAMS\n",
    "        self.BETA = beta\n",
    "        self.beta_max = beta_max\n",
    "        self.beta_increment = beta_increment\n",
    "        self.mem_size = mem_size\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = ReplayBuffer(mem_size, observation_shape, n_actions, mem_alpha)\n",
    "\n",
    "        # MODEL PARAMS\n",
    "        self.warmup = warmup\n",
    "        self.move_counter = 0\n",
    "        self.learn_counter = 0 # TO UPDATE TARGET NETWORK\n",
    "        self.algo_name = algo_name\n",
    "        self.env_name = env_name\n",
    "        self.model_dir = model_dir\n",
    "        self.Q_TARGET_replace_interval = Q_TARGET_replace_interval\n",
    "        # Q1\n",
    "        self.Q_STEP = DuelingDeepQNetwork(lr, observation_shape, n_actions,\n",
    "                              model_name = env_name+'_'+algo_name+'_Q_STEP',\n",
    "                              model_dir = model_dir)\n",
    "        # Q2\n",
    "        self.Q_TARGET = DuelingDeepQNetwork(lr, observation_shape, n_actions,\n",
    "                              model_name = env_name+'_'+algo_name+'_Q_TARGET',\n",
    "                              model_dir = model_dir)\n",
    "\n",
    "    # e-GREEDY POLICY\n",
    "    def get_action(self, observation, greedy=False):\n",
    "        if ( (np.random.uniform() >= self.EPSILON) or greedy):\n",
    "            observation = T.tensor(observation, dtype=T.float32).to(self.Q_STEP.device)\n",
    "            state = T.unsqueeze(observation, 0)\n",
    "            _,A = self.Q_STEP(state)\n",
    "            action = T.argmax(A).item()\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        return action\n",
    "\n",
    "    def learn(self):\n",
    "        if (self.move_counter < self.warmup): return # return if not explored enough\n",
    "        if (self.memory.mem_counter < self.batch_size): return # return if insufficient samples present\n",
    "        # RESET TARGET NETWORK (every 1000 steps)\n",
    "        self.update_Q_TARGET()\n",
    "\n",
    "        self.learn_counter += 1\n",
    "        batch_indices, states, actions, rewards, states_, terminals, importance = self.sample_batch()\n",
    "        # PREDICT Q1(s,a)\n",
    "        v1,a1 = self.Q_STEP(states)\n",
    "        q1 = v1 + (a1 - a1.mean(dim=1, keepdim=True)) # q - batch_size * n_actions\n",
    "        indices = np.arange(len(actions))\n",
    "        q1_preds = q1[indices,actions]\n",
    "\n",
    "        # GET V1,A2(s_,A) and V2,A2(s_,A)\n",
    "        v1_, a1_ = self.Q_STEP(states_)\n",
    "        v2_, a2_ = self.Q_TARGET(states_)\n",
    "        # GET Q1(s_,A) and Q2(s_,A)\n",
    "        q1_ = v1_ + (a1_ - a1_.mean(dim=1, keepdim=True))\n",
    "        q2_ = v2_ + (a2_ - a2_.mean(dim=1, keepdim=True))\n",
    "        # argmax(Q1(s_,A)) - (max)a_\n",
    "        # Q2(s_, (max)a_) - TARGETS\n",
    "        a_ = T.argmax(q1_, dim=1)\n",
    "        indices = np.arange(len(a_))\n",
    "        q2_next = q2_[indices, a_]\n",
    "        q2_next[terminals] = 0.0                      # Q2(s_) = 0 where terminal=1\n",
    "        q2_targets = rewards + (self.GAMMA * q2_next)\n",
    "\n",
    "        # CALC LOSS & BACKPROP\n",
    "        errors = (q1_preds - q2_targets)\n",
    "        loss = ((errors ** 2) * importance)\n",
    "        loss = loss.mean()\n",
    "\n",
    "        self.Q_STEP.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.Q_STEP.optimizer.step()\n",
    "\n",
    "        self.decay_epsilon()\n",
    "        self.increment_beta()\n",
    "        self.memory.update_priotities(batch_indices, errors.cpu().detach().numpy(), offset=0.1)\n",
    "\n",
    "    def update_Q_TARGET(self):\n",
    "        if ((self.learn_counter % self.Q_TARGET_replace_interval) == 0):\n",
    "            self.Q_TARGET.load_state_dict(self.Q_STEP.state_dict())\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        if (self.EPSILON > self.epsilon_min):\n",
    "            self.EPSILON -= self.epsilon_decay\n",
    "        else:\n",
    "            self.EPSILON = self.epsilon_min\n",
    "\n",
    "    def increment_beta(self):\n",
    "        if(self.BETA < self.beta_max):\n",
    "            self.BETA += self.beta_increment\n",
    "        else:\n",
    "            self.BETA = self.beta_max\n",
    "    \n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        self.memory.store_transition(state, action, reward, state_, done)\n",
    "\n",
    "    def sample_batch(self):\n",
    "        batch_indices, states, actions, rewards, states_, terminals, importance = self.memory.sample_batch(self.batch_size, self.BETA)\n",
    "        states = T.tensor(states).to(self.Q_STEP.device)\n",
    "        actions = T.tensor(actions).to(self.Q_STEP.device)\n",
    "        rewards = T.tensor(rewards).to(self.Q_STEP.device)\n",
    "        states_ = T.tensor(states_).to(self.Q_STEP.device)\n",
    "        terminals = T.tensor(terminals).to(self.Q_STEP.device)\n",
    "        importance = T.tensor(importance).to(self.Q_STEP.device)\n",
    "        return batch_indices, states, actions, rewards, states_, terminals, importance\n",
    "        \n",
    "    def save_models(self):\n",
    "        self.Q_STEP.save_model()\n",
    "        self.Q_TARGET.save_model()\n",
    "    \n",
    "    def load_models(self, cpu=False):\n",
    "        self.Q_STEP.load_model(cpu)\n",
    "        self.Q_TARGET.load_model(cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g7HQrV3WcH2c"
   },
   "source": [
    "## **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mj6YZi0aXsDX"
   },
   "outputs": [],
   "source": [
    "env_name = \"LunarLander-v2\"\n",
    "env = gym.make(env_name)\n",
    "\n",
    "N_EPISODES = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GRpSgQ0OJi9d"
   },
   "outputs": [],
   "source": [
    "agent = DuelingDDQNAgent(observation_shape=env.observation_space.shape,\n",
    "                         n_actions=env.action_space.n,\n",
    "                         lr=1e-4,\n",
    "                         gamma=0.99,\n",
    "                         epsilon=1.0,\n",
    "                         epsilon_min=0.01,\n",
    "                         epsilon_decay=1e-5,\n",
    "                         beta=0.4,\n",
    "                         beta_max=1.0,\n",
    "                         beta_increment=1e-4,\n",
    "                         mem_size=20000,\n",
    "                         mem_alpha=0.6,\n",
    "                         batch_size=64,\n",
    "                         Q_TARGET_replace_interval=1000,\n",
    "                         warmup = 100,\n",
    "                         algo_name='DuelingDDQN',\n",
    "                         env_name=env_name,\n",
    "                         model_dir='./weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "b3c36bbc608749bbb8779f40288a628d",
      "25d40efe526a4c7e96c085225b06ed6b",
      "b9d381c29299482780440a3353c8a5de",
      "cfa7676279414028876ee5c07c88731e",
      "3f513ece9b26424eae4e1dea8c68e28c",
      "e51ea438e13d4db7bb178d173980b523",
      "4d0ff2140303430a98fac26f34fbac1e",
      "eee2b8170c834168ad3eb227b53267cc"
     ]
    },
    "colab_type": "code",
    "id": "Rp2We9miezCT",
    "outputId": "2d8c0526-262c-4879-bcf1-a943a2fcf593"
   },
   "outputs": [],
   "source": [
    "episode_rewards, episode_lengths, episode_epsilons, mean_rewards = [],[],[],[]\n",
    "best_reward = -np.inf\n",
    "\n",
    "for episode_n in tqdm(range(N_EPISODES)):\n",
    "    total_reward, total_moves = 0,0\n",
    "\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "\n",
    "    while not done:\n",
    "        agent.move_counter+=1\n",
    "        # e_GREEDY ACTION\n",
    "        action = agent.get_action(observation)\n",
    "        observation_, reward, done, _ = env.step(action)\n",
    "\n",
    "        total_reward += reward\n",
    "        total_moves += 1\n",
    "\n",
    "        # STORE DATA & LEARN\n",
    "        agent.store_transition(observation, action, reward, observation_, done)\n",
    "        agent.learn()\n",
    "\n",
    "        observation = observation_\n",
    "\n",
    "    episode_rewards.append(total_reward)\n",
    "    episode_lengths.append(total_moves)\n",
    "    episode_epsilons.append(agent.EPSILON)\n",
    "\n",
    "    mean_reward = np.mean(episode_rewards[-100:])\n",
    "    mean_rewards.append(mean_reward)\n",
    "    if(mean_reward > best_reward):\n",
    "        agent.save_models()\n",
    "        best_reward = mean_reward\n",
    "\n",
    "    print(\"ITER: \",episode_n,\"\\tRWD: \",total_reward,\"\\tM_RWD: \",round(mean_reward,2),\"\\tLEN: \",total_moves,\"\\tEPS: \",round(agent.EPSILON,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "H8jwhRp6M8pj",
    "outputId": "72d77dcd-9823-4b3c-b0b8-f0ed275e47cf"
   },
   "outputs": [],
   "source": [
    "plt.plot(mean_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yMBRuS2vB-3q"
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[INFO] Loading model\n[INFO] Loading model\n"
    }
   ],
   "source": [
    "env_name = \"LunarLander-v2\"\n",
    "env = gym.make(env_name)\n",
    "\n",
    "agent = DuelingDDQNAgent(observation_shape=env.observation_space.shape,\n",
    "                         n_actions=env.action_space.n,\n",
    "                         lr=1e-4,\n",
    "                         gamma=0.99,\n",
    "                         epsilon=1.0,\n",
    "                         epsilon_min=0.01,\n",
    "                         epsilon_decay=1e-5,\n",
    "                         beta=0.4,\n",
    "                         beta_max=1.0,\n",
    "                         beta_increment=1e-4,\n",
    "                         mem_size=1,\n",
    "                         mem_alpha=0.6,\n",
    "                         batch_size=1,\n",
    "                         Q_TARGET_replace_interval=1000,\n",
    "                         initial_exploration_steps = 10,\n",
    "                         algo_name='DuelingDDQN',\n",
    "                         env_name=env_name,\n",
    "                         model_dir='./weights')\n",
    "\n",
    "agent.load_models(cpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "RWD:  157.91487961932543 \tLEN:  1000\n"
    }
   ],
   "source": [
    "with T.no_grad():\n",
    "    total_reward, total_moves = 0,0\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "\n",
    "    while not done:\n",
    "        time.sleep(0.0001)\n",
    "        env.render()\n",
    "\n",
    "        # e_GREEDY ACTION\n",
    "        action = agent.get_action(observation, greedy=True)\n",
    "        observation_, reward, done, _ = env.step(action)\n",
    "\n",
    "        total_reward += reward\n",
    "        total_moves += 1\n",
    "\n",
    "        observation = observation_\n",
    "    print(\"RWD: \",total_reward,\"\\tLEN: \",total_moves)\n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "j4SkkGZU9Dd_",
    "B9mN3hsv9Hl2",
    "APhMTaPP9Kxl"
   ],
   "name": "LunarLander.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "25d40efe526a4c7e96c085225b06ed6b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3f513ece9b26424eae4e1dea8c68e28c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "4d0ff2140303430a98fac26f34fbac1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b3c36bbc608749bbb8779f40288a628d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b9d381c29299482780440a3353c8a5de",
       "IPY_MODEL_cfa7676279414028876ee5c07c88731e"
      ],
      "layout": "IPY_MODEL_25d40efe526a4c7e96c085225b06ed6b"
     }
    },
    "b9d381c29299482780440a3353c8a5de": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": " 49%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e51ea438e13d4db7bb178d173980b523",
      "max": 3000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3f513ece9b26424eae4e1dea8c68e28c",
      "value": 1483
     }
    },
    "cfa7676279414028876ee5c07c88731e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eee2b8170c834168ad3eb227b53267cc",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_4d0ff2140303430a98fac26f34fbac1e",
      "value": " 1483/3000 [41:58&lt;40:59,  1.62s/it]"
     }
    },
    "e51ea438e13d4db7bb178d173980b523": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eee2b8170c834168ad3eb227b53267cc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}