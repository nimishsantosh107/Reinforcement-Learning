{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DQN.ipynb","provenance":[],"collapsed_sections":["9u_bVI_99Rfv","j4SkkGZU9Dd_","B9mN3hsv9Hl2","APhMTaPP9Kxl"],"toc_visible":true,"authorship_tag":"ABX9TyNoAQ4Rpbc2gTJZJ+hdiCPH"},"kernelspec":{"name":"python_defaultSpec_1596826856201","display_name":"Python 3.6.10 64-bit ('reinforcement-learning': conda)"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"553f3f2b4e164ccfbc5ddfd2d79bcdfa":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c74e563a80954025bf54f09f6ddab1ef","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_800189801a7b4df081c2d1212396f440","IPY_MODEL_aa8baad31c564581a9bb59e9246401be"]}},"c74e563a80954025bf54f09f6ddab1ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"800189801a7b4df081c2d1212396f440":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_8097a02df891400c914eb8d624256c91","_dom_classes":[],"description":" 48%","_model_name":"FloatProgressModel","bar_style":"","max":500,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":242,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0b41cf3738f94d26b2b02c8caf929843"}},"aa8baad31c564581a9bb59e9246401be":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_db086016d7fb441b91078372660ae2df","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 242/500 [2:41:18&lt;2:53:42, 40.40s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_709e6b240ab145f5a14d30a2955cfb2c"}},"8097a02df891400c914eb8d624256c91":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0b41cf3738f94d26b2b02c8caf929843":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"db086016d7fb441b91078372660ae2df":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"709e6b240ab145f5a14d30a2955cfb2c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"KLAOgs3gOPy8","colab_type":"code","colab":{}},"source":["import os\n","import time\n","import cv2\n","import gym\n","import collections\n","\n","import numpy as np\n","import torch as T\n","import torch.optim as optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","\n","from tqdm.notebook import tqdm\n","\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rZDPJ0g-e6fh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1596816577313,"user_tz":-330,"elapsed":1619,"user":{"displayName":"Nimish S","photoUrl":"","userId":"16860995465865824316"}},"outputId":"25bd6f03-aade-4d43-f9aa-524ecee5265b"},"source":["# T.cuda.get_device_name()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9u_bVI_99Rfv","colab_type":"text"},"source":["## **Wrappers**"]},{"cell_type":"code","metadata":{"id":"L5aHWPN-OEAJ","colab_type":"code","colab":{}},"source":["# PREPROCESS EACH FRAME\n","class PreprocessFrames(gym.ObservationWrapper):\n","    \"\"\"\n","    PREPROCESSES EACH FRAME (input = (rows, columns, 3)) [0-255]\n","    1. convert to grayscale (3 channels to 1)   -   (rows, columns, 1)  [0-255]\n","    2. resize to new shape                      -   (new_rows, new_columns)  [0-255]\n","    3. convert to nparray & reshape             -   array(1, new_rows, new_columns)  [0-255]\n","    4. scale values from 0-1                    -   array(1, new_rows, new_columns)  [0.0-1.0]\n","    \"\"\"\n","    def __init__(self, env, new_observation_shape):\n","        super().__init__(env)\n","        self.new_observation_shape = new_observation_shape\n","        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=self.new_observation_shape, dtype=np.float32)\n","    \n","    def observation(self, observation):\n","        temp_frame = cv2.cvtColor(observation, cv2.COLOR_RGB2GRAY)\n","        temp_frame = cv2.resize(temp_frame, self.new_observation_shape[1:], interpolation=cv2.INTER_AREA)\n","        new_observation = np.array(temp_frame).reshape(self.new_observation_shape)\n","        new_observation = new_observation / 255.0 \n","        return new_observation\n","\n","# TO BE CALLED ON EACH SINGLE IMAGE (AFTER PREPROCESS)\n","class CustomStep(gym.Wrapper):\n","    \"\"\"\n","    OVERRIDES step() & reset()\n","    1. repeats same action in 'n' skipped frames to compute faster.\n","    2. removes flicker in frames by taking max of 2 consecutive frames.\n","    \"\"\"\n","    def __init__(self, env, frame_skip, clip_reward, no_ops, fire_first):\n","        super().__init__(env)\n","        self.frame_skip = frame_skip\n","        self.observation_shape = env.observation_space.shape\n","        self.observation_buffer = np.zeros_like((2, self.observation_shape))\n","        # DURING TESTING ONLY\n","        self.clip_reward = clip_reward\n","        self.no_ops = no_ops\n","        self.fire_first = fire_first\n","\n","    def reset(self):\n","        observation = self.env.reset()\n","        # FOR no_ops\n","        no_ops = (np.random.randint(self.no_ops) + 1) if (self.no_ops > 0) else 0\n","        for _ in range(no_ops):\n","            _, _, done, _ = env.step(0) # 0 - NOOP\n","            if done: self.env.reset()\n","        # FOR fire_first\n","        if (self.fire_first):\n","            assert (self.env.get_action_meanings()[0] == 'FIRE')\n","            observation, _, _, _ = env.step(1) # 1 - FIRE\n","        self.observation_buffer = np.zeros_like((2, self.observation_shape))\n","        self.observation_buffer[0] = observation\n","        return observation\n","\n","    # RETURN FRAME_SKIPPED & FLICKER REMOVED FRAMES \n","    def step(self, action):\n","        total_reward = 0.0\n","        done = False\n","\n","        for frame in range(self.frame_skip):\n","            observation, reward, done, info = self.env.step(action)\n","            # CLIP REWARD (-1,1) IF true\n","            reward = reward if (not self.clip_reward) else np.clip(reward, -1,1)\n","            total_reward += reward\n","\n","            idx = frame % 2\n","            self.observation_buffer[idx] = observation\n","\n","            if done: break\n","\n","        observation_max = np.maximum(self.observation_buffer[0], self.observation_buffer[1])\n","        return observation_max, total_reward, done, info\n","\n","\n","# STACK OBSERVATIONS\n","class StackFrames(gym.ObservationWrapper):\n","    \"\"\"\n","    STACKS stack_size FRAMES TOGETHER AND RETURNS AS THE 'observation'\n","    1. on reset() returns first 'observation' STACKED 'stack_size' times\n","    2. observation() returns current 'observation' STACKED with 'stack_size-1' previous 'observation'\n","    \"\"\"\n","    def __init__(self, env, stack_size):\n","        super().__init__(env)\n","        self.observation_space = gym.spaces.Box(\n","                                    env.observation_space.low.repeat(stack_size, axis=0),\n","                                    env.observation_space.high.repeat(stack_size, axis=0)\n","                                 )\n","        self.stack = collections.deque(maxlen=stack_size)\n","\n","    def reset(self):\n","        self.stack.clear()\n","        observation = self.env.reset()\n","        for _ in range(self.stack.maxlen):\n","            self.stack.append(observation)\n","        observation = np.array(self.stack).reshape(self.observation_space.shape)\n","        return observation\n","        \n","    def observation(self, observation):\n","        self.stack.append(observation)\n","        observation = np.array(self.stack).reshape(self.observation_space.shape)\n","        return observation"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R9YVrXJ0A0nd","colab_type":"code","colab":{}},"source":["# TIE EVERYTHING TOGETHER\n","def make_env(env_name, new_observation_shape=(1,84,84), stack_size=4, frame_skip=4, clip_reward=False, no_ops=0, fire_first=False):\n","    env = gym.make(env_name)\n","    env = PreprocessFrames(env, new_observation_shape=new_observation_shape)\n","    env = CustomStep(env, frame_skip=4, clip_reward=clip_reward, no_ops=no_ops, fire_first=fire_first)\n","    env = StackFrames(env, stack_size=stack_size)\n","    return env"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j4SkkGZU9Dd_","colab_type":"text"},"source":["## **ReplayBuffer**"]},{"cell_type":"code","metadata":{"id":"-SvHE9YKo0r-","colab_type":"code","colab":{}},"source":["class ReplayBuffer:\n","    def __init__(self, mem_size, observation_shape, n_actions):\n","        self.mem_size = mem_size\n","        self.mem_counter = 0\n","        # DATA\n","        self.states = np.zeros((mem_size, *observation_shape), dtype=np.float32)\n","        self.actions = np.zeros(mem_size, dtype=np.int64)\n","        self.rewards = np.zeros(mem_size, dtype=np.int64)\n","        self.states_ = np.zeros((mem_size, *observation_shape), dtype=np.float32)\n","        self.terminals = np.zeros(mem_size, dtype=bool)\n","\n","    # STORE TRANSITIONS IN BUFFER\n","    def store_transition(self, state, action, reward, state_, done):\n","        index = self.mem_counter % self.mem_size\n","        self.states[index] = state\n","        self.actions[index] = action\n","        self.rewards[index] = reward\n","        self.states_[index] = state_\n","        self.terminals[index] = done # 1 if 'done' else 0\n","        self.mem_counter += 1\n","\n","    # UNIFORMLY SAMPLES 'BUFFER' AND RETURNS A 'BATCH' OF batch_size\n","    def sample_batch(self, batch_size):\n","        max_index = min(self.mem_counter, self.mem_size)\n","        batch_indices = np.random.choice(max_index, batch_size, replace=False)\n","        states = self.states[batch_indices]\n","        actions = self.actions[batch_indices]\n","        rewards = self.rewards[batch_indices]\n","        states_ = self.states_[batch_indices]\n","        terminals = self.terminals[batch_indices]\n","        return (states, actions, rewards, states_, terminals)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B9mN3hsv9Hl2","colab_type":"text"},"source":["## **Network**"]},{"cell_type":"code","metadata":{"id":"N4-yaJCQ0Kci","colab_type":"code","colab":{}},"source":["class DuelingDeepQNetwork(nn.Module):\n","    def __init__(self, lr, observation_shape, n_actions, model_name, model_dir):\n","        super().__init__()\n","        self.model_dir = model_dir\n","        self.model_file = os.path.join(self.model_dir, model_name)\n","        # CNN\n","        self.conv1 = nn.Conv2d(observation_shape[0], 32, kernel_size=8, stride=4)\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n","        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n","        # CNN -> ANN\n","        fc_input_dims = self.caculate_conv_output_dims(observation_shape)\n","        # ANN\n","        self.fc1 = nn.Linear(fc_input_dims, 512)\n","        # DUELING\n","        self.V = nn.Linear(512, 1)\n","        self.A = nn.Linear(512, n_actions)\n","        # UTILS\n","        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n","        self.optimizer = optim.RMSprop(self.parameters(), lr=lr)\n","        self.loss = nn.MSELoss()\n","        self.to(self.device)\n","    \n","    def forward(self, state):\n","        t = F.relu(self.conv1(state))\n","        t = F.relu(self.conv2(t))\n","        t = F.relu(self.conv3(t))\n","        t = F.relu(self.fc1(t.reshape(t.shape[0], -1)))\n","        V = self.V(t)\n","        A = self.A(t)\n","        return V,A\n","\n","    def caculate_conv_output_dims(self, observation_shape):\n","        dims = T.zeros((1, *observation_shape))\n","        dims = self.conv1(dims)\n","        dims = self.conv2(dims)\n","        dims = self.conv3(dims)\n","        return int(np.prod(dims.shape))\n","\n","    def save_model(self):\n","        print(\"[INFO] Saving model\")\n","        checkpoint = {\n","            'model_state_dict': self.state_dict(),\n","            'optimizer_state_dict' : self.optimizer.state_dict()\n","        }\n","        T.save(checkpoint, self.model_file)\n","    \n","    def load_model(self, cpu=False):\n","        print(\"[INFO] Loading model\")\n","        \n","        map_location = T.device('cpu') if (cpu) else None\n","        \n","        checkpoint = T.load(self.model_file, map_location=map_location)\n","        self.load_state_dict(checkpoint['model_state_dict'])\n","        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"APhMTaPP9Kxl","colab_type":"text"},"source":["## **Agent**"]},{"cell_type":"code","metadata":{"id":"Jf105mZ2pb8I","colab_type":"code","colab":{}},"source":["class DuelingDQNAgent:\n","    def __init__(self, observation_shape, n_actions, lr, gamma, epsilon, epsilon_min, epsilon_decay,\n","                 mem_size, batch_size, Q_TARGET_replace_interval, algo_name, env_name, model_dir):\n","        self.observation_shape = observation_shape\n","        self.n_actions = n_actions\n","        self.LR = lr\n","        self.GAMMA = gamma\n","        self.EPSILON = epsilon\n","        self.epsilon_min = epsilon_min\n","        self.epsilon_decay = epsilon_decay\n","\n","        # MEM PARAMS\n","        self.mem_size = mem_size\n","        self.batch_size = batch_size\n","        self.memory = ReplayBuffer(mem_size, observation_shape, n_actions)\n","\n","        # MODEL PARAMS\n","        self.learn_counter = 0 # TO UPDATE TARGET NETWORK\n","        self.algo_name = algo_name\n","        self.env_name = env_name\n","        self.model_dir = model_dir\n","        self.Q_TARGET_replace_interval = Q_TARGET_replace_interval\n","        # Q1\n","        self.Q_STEP = DuelingDeepQNetwork(lr, observation_shape, n_actions,\n","                              model_name = algo_name+'_Q_STEP',\n","                              model_dir = model_dir)\n","        # Q2\n","        self.Q_TARGET = DuelingDeepQNetwork(lr, observation_shape, n_actions,\n","                              model_name = algo_name+'_Q_TARGET',\n","                              model_dir = model_dir)\n","\n","    # e-GREEDY POLICY\n","    def get_action(self, observation, greedy=False):\n","        if ( (np.random.uniform() >= self.EPSILON) or greedy):\n","            observation = T.tensor(observation, dtype=T.float32).to(self.Q_STEP.device)\n","            state = T.unsqueeze(observation, 0)\n","            _,A = self.Q_STEP(state)\n","            action = T.argmax(A).item()\n","        else:\n","            action = env.action_space.sample()\n","        return action\n","\n","    def learn(self):\n","        if (self.memory.mem_counter < self.batch_size): return # return if insufficient samples present\n","        # RESET TARGET NETWORK (1 / 1000)\n","        self.update_Q_TARGET()\n","\n","        states, actions, rewards, states_, terminals = self.sample_batch()\n","        # PREDICT Q1(s,a)\n","        v1,a1 = self.Q_STEP(states)\n","        q1 = v1 + (a1 - a1.mean(dim=1, keepdim=True))\n","        indices = np.arange(len(actions))\n","        q1_preds = q1[indices,actions]\n","\n","        # GET Q2(s_,a_) WHERE a_ = max(Q2(s_, A))\n","        v2_, a2_ = self.Q_TARGET(states_)\n","        q2_ = v2_ + (a2_ - a2_.mean(dim=1, keepdim=True))\n","        q2_next = (q2_.max(dim=1))[0]       # MAX VAL ACTION (without added reward)\n","        q2_next[terminals] = 0.0            # Q(s_) = 0 where terminal=1\n","        q2_targets = rewards + (self.GAMMA * q2_next)\n","\n","        # CALC LOSS & BACKPROP\n","        loss = self.Q_STEP.loss(q2_targets, q1_preds).to(self.Q_STEP.device)\n","        self.Q_STEP.optimizer.zero_grad()\n","        loss.backward()\n","        self.Q_STEP.optimizer.step()\n","\n","        self.learn_counter += 1\n","        self.decay_epsilon()\n","\n","    def update_Q_TARGET(self):\n","        if ((self.learn_counter % self.Q_TARGET_replace_interval) == 0):\n","            self.Q_TARGET.load_state_dict(self.Q_STEP.state_dict())\n","    \n","    def decay_epsilon(self):\n","        if (self.EPSILON > self.epsilon_min):\n","            self.EPSILON -= self.epsilon_decay\n","        else:\n","            self.EPSILON = self.epsilon_min\n","    \n","    def store_transition(self, state, action, reward, state_, done):\n","        self.memory.store_transition(state, action, reward, state_, done)\n","\n","    def sample_batch(self):\n","        states, actions, rewards, states_, terminals = self.memory.sample_batch(self.batch_size)\n","        states = T.tensor(states).to(self.Q_STEP.device)\n","        actions = T.tensor(actions).to(self.Q_STEP.device)\n","        rewards = T.tensor(rewards).to(self.Q_STEP.device)\n","        states_ = T.tensor(states_).to(self.Q_STEP.device)\n","        terminals = T.tensor(terminals).to(self.Q_STEP.device)\n","        return states, actions, rewards, states_, terminals\n","        \n","    def save_models(self):\n","        self.Q_STEP.save_model()\n","        self.Q_TARGET.save_model()\n","    \n","    def load_models(self, cpu=False):\n","        self.Q_STEP.load_model(cpu)\n","        self.Q_TARGET.load_model(cpu)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g7HQrV3WcH2c","colab_type":"text"},"source":["## **Training**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## TRAINING ##"]},{"cell_type":"code","metadata":{"id":"mj6YZi0aXsDX","colab_type":"code","colab":{}},"source":["env_name = 'PongNoFrameskip-v4'\n","env = make_env(env_name)\n","\n","N_EPISODES = 300\n","\n","agent = DuelingDQNAgent(observation_shape=env.observation_space.shape,\n","                  n_actions=env.action_space.n,\n","                  lr=1e-4,\n","                  gamma=0.99,\n","                  epsilon=1.0,\n","                  epsilon_min=0.06,\n","                  epsilon_decay=1e-5,\n","                  mem_size=25000,\n","                  batch_size=128,\n","                  Q_TARGET_replace_interval=1000,\n","                  algo_name='DuelingDQN',\n","                  env_name=env_name,\n","                  model_dir='./')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rp2We9miezCT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["553f3f2b4e164ccfbc5ddfd2d79bcdfa","c74e563a80954025bf54f09f6ddab1ef","800189801a7b4df081c2d1212396f440","aa8baad31c564581a9bb59e9246401be","8097a02df891400c914eb8d624256c91","0b41cf3738f94d26b2b02c8caf929843","db086016d7fb441b91078372660ae2df","709e6b240ab145f5a14d30a2955cfb2c"]},"outputId":"103b09cc-6906-410f-cd36-2fb46bcc1869"},"source":["episode_rewards, episode_lengths, episode_epsilons, mean_rewards = [],[],[],[]\n","best_reward = -np.inf\n","\n","for episode_n in tqdm(range(N_EPISODES)):\n","    total_reward, total_moves = 0,0\n","\n","    done = False\n","    observation = env.reset()\n","\n","    while not done:\n","        # e_GREEDY ACTION\n","        action = agent.get_action(observation)\n","        observation_, reward, done, _ = env.step(action)\n","\n","        total_reward += reward\n","        total_moves += 1\n","\n","        # STORE DATA & LEARN\n","        agent.store_transition(observation, action, reward, observation_, done)\n","        agent.learn()\n","\n","        observation = observation_\n","\n","    episode_rewards.append(total_reward)\n","    episode_lengths.append(total_moves)\n","    episode_epsilons.append(agent.EPSILON)\n","\n","    mean_reward = np.mean(episode_rewards[-100:])\n","    mean_rewards.append(mean_reward)\n","    if(mean_reward > best_reward):\n","        agent.save_models()\n","        best_reward = mean_reward\n","\n","    print(\"ITER: \",episode_n,\"\\tRWD: \",total_reward,\"\\tMEAN_RWD: \",round(mean_reward,2),\"\\tLEN: \",total_moves,\"\\tEPS: \",round(agent.EPSILON,4))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{},"source":["# Testing"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["env_name = 'PongNoFrameskip-v4'\n","env = make_env(env_name)\n","\n","agent = DuelingDQNAgent(observation_shape=env.observation_space.shape,\n","                 n_actions=env.action_space.n,\n","                 lr=1e-4,\n","                 gamma=0.99,\n","                 epsilon=0.001,\n","                 epsilon_min=0.001,\n","                 epsilon_decay=1e-5,\n","                 mem_size=1,\n","                 batch_size=1,\n","                 Q_TARGET_replace_interval=1000,\n","                 algo_name='DuelingDQN',\n","                 env_name=env_name,\n","                 model_dir='./')"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[]},"outputs":[],"source":["agent.load_models(cpu=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[]},"outputs":[],"source":["with T.no_grad():\n","    total_reward, total_moves = 0,0\n","    done = False\n","    observation = env.reset()\n","\n","    while not done:\n","        time.sleep(0.0001)\n","        env.render()\n","\n","        # e_GREEDY ACTION\n","        action = agent.get_action(observation, greedy=True)\n","        observation_, reward, done, _ = env.step(action)\n","\n","        total_reward += reward\n","        total_moves += 1\n","\n","        observation = observation_\n","    print(\"RWD: \",total_reward,\"\\tLEN: \",total_moves)\n","    env.close()"]}]}